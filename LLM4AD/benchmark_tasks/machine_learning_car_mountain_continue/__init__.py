#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Autonomous LLM4AD task: machine_learning_car_mountain_continue
Generated by convert_llm4ad_benchmark.py

This is a fully self-contained task module that doesn't depend on the original LLM4AD codebase.
"""

# Embedded evaluation code (benchmark)
# Module Name: CarMountainCEvaluation
# Last Revision: 2025/3/5
# Description: Designs a heuristic strategy function for controlling a car along an uneven road (Continuous Mountain Car problem).
#              The function applies an appropriate force based on the car's position and velocity to guide the car
#              towards a target in the minimum number of steps.
#              This module is part of the LLM4AD project (https://github.com/Optima-CityU/llm4ad).
#
# Parameters:
#    -   position: float - Car's position, range [-1.2, 0.6] (default: None).
#    -   velocity: float - Car's velocity, range [-0.07, 0.07] (default: None).
#    -   last_action: float - Car's last applied force, range [-1.0, 1.0] (default: None).
#    -   timeout_seconds: int - Maximum allowed time (in seconds) for the evaluation process (default: 20).
#
# References:
#   - Brockman, Greg, et al. "Openai gym." arXiv preprint arXiv:1606.01540 (2016).
#
# ------------------------------- Copyright --------------------------------
# Copyright (c) 2025 Optima Group.
#
# Permission is granted to use the LLM4AD platform for research purposes.
# All publications, software, or other works that utilize this platform
# or any part of its codebase must acknowledge the use of "LLM4AD" and
# cite the following reference:
#
# Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang,
# Zhichao Lu, and Qingfu Zhang, "LLM4AD: A Platform for Algorithm Design
# with Large Language Model," arXiv preprint arXiv:2412.17287 (2024).
#
# For inquiries regarding commercial use or licensing, please contact
# http://www.llm4ad.com/contact.html
# --------------------------------------------------------------------------

from __future__ import annotations

from typing import Any
import os, sys
sys.path.insert(0, os.path.dirname(__file__))
import gym

from llm4ad_loader import Evaluation
# from llm4ad.task.machine_learning.car_mountain_continue.template import template_program, task_description  # Template values embedded below

# Embedded template values
template_program = 'import numpy as np\n\ndef choose_action(pos: float, v: float, last_action: float) -> float:\n    """Return the action for the car to proceed the next move.\n    Args:\n        pos: Car\'s position, a float ranges between [-1.2, 0.6].\n        v: Car\'s velocity, a float ranges between [-0.07, 0.07].\n        last_action: Car\'s next move, a float ranges between [-1, 1].\n    Return:\n         A [float] representing the force to be applied to the car.\n         The value should be in the range of [-1.0, 1.0].\n    """\n    return np.random.uniform(-1.0, 1.0)'
task_description = '("Implement a function that designing a novel strategy function that guide the car along an uneven "'



__all__ = ['CarMountainCEvaluation']


def evaluate(env: gym.Env, action_select: callable) -> float:
    """Evaluate heuristic function on car mountain problem."""

    observation, _ = env.reset()  # initialization

    action = 0  # initial action, stay static

    for i in range(env._max_episode_steps):
        action = action_select(observation[0], observation[1], action)
        observation, reward, done, truncated, info = env.step([action])

        if done:
            return -(i / env._max_episode_steps)  # succeed

        if truncated:
            return -(max(0.5 - observation[0], 0) + 1)  # failed



class CarMountainCEvaluation(Evaluation):
    """Evaluator for car mountain problem."""

    def __init__(self, max_steps=500, timeout_seconds=20, **kwargs):
        """
            Args:
                - 'max_steps' (int): Maximum number of steps allowed per episode in the MountainCar-v0 environment (default is 500).
                - '**kwargs' (dict): Additional keyword arguments passed to the parent class initializer.

            Attributes:
                - 'env' (gym.Env): The MountainCar-v0 environment with a modified maximum episode length.
        """

        super().__init__(
            template_program=template_program,
            task_description=task_description,
            use_numba_accelerate=False,
            timeout_seconds=timeout_seconds
        )

        self.env = None
        self.env = gym.make('MountainCarContinuous-v0')
        self.env._max_episode_steps = max_steps

    def evaluate_program(self, program_str: str, callable_func: callable) -> Any | None:
        try:
            a = evaluate(self.env, callable_func)
        except Exception as e:
            print(e)
        return evaluate(self.env, callable_func)

# Task configuration for benchmark task
ENTRY_NAME = 'choose_action'
FUNCTION_SIGNATURE = 'def choose_action(...):'
IMPORT_HEADER = 'import numpy as np\nimport math'
TASK_DESCRIPTION = '("Implement a function that designing a novel strategy function that guide the car along an uneven "'
OBJECTIVE_TEXT = 'You are optimizing the implementation of `choose_action` for the LLM4AD task.\\n\\nTask description:\\n("Implement a function that designing a novel strategy function that guide the car along an uneven "\\n\\nYour goal is to return a correct and efficient function whose score (computed by the task evaluator) is as high as possible.'
TEMPLATE_FUNCTION = 'import numpy as np\n\ndef choose_action(pos: float, v: float, last_action: float) -> float:\n    """Return the action for the car to proceed the next move.\n    Args:\n        pos: Car\'s position, a float ranges between [-1.2, 0.6].\n        v: Car\'s velocity, a float ranges between [-0.07, 0.07].\n        last_action: Car\'s next move, a float ranges between [-1, 1].\n    Return:\n         A [float] representing the force to be applied to the car.\n         The value should be in the range of [-1.0, 1.0].\n    """\n    return np.random.uniform(-1.0, 1.0)'
EVAL_CLASS_NAME = 'CarMountainCEvaluation'
EVAL_KWARGS = {'max_steps': 500, 'timeout_seconds': 20}

def build_trace_problem(**override_eval_kwargs) -> dict:
    """Build a Trace-ready problem using embedded benchmark evaluator."""
    
    # Create evaluator instance with embedded class
    eval_kwargs_final = EVAL_KWARGS.copy()
    eval_kwargs_final.update(override_eval_kwargs)
    
    evaluator = globals()[EVAL_CLASS_NAME](**eval_kwargs_final)
    
    from llm4ad_loader import AutonomousEvaluatorGuide
    from opto import trace
    
    # Create parameter
    initial_code = TEMPLATE_FUNCTION.strip()
    param = trace.node(initial_code, name='__code', 
                      description=f'The code should start with: {FUNCTION_SIGNATURE}', 
                      trainable=True)
    
    # Create guide using benchmark embedded evaluator
    guide = AutonomousEvaluatorGuide(evaluator, ENTRY_NAME, IMPORT_HEADER, 
                                   timeout=eval_kwargs_final.get('timeout_seconds', 30))
    
    # Create dataset
    train_dataset = dict(
        inputs=[TASK_DESCRIPTION],
        infos=[{'imports': IMPORT_HEADER, 'entry': ENTRY_NAME}]
    )
    
    # Optimizer kwargs
    optimizer_kwargs = dict(
        objective=OBJECTIVE_TEXT,
        memory_size=10
    )
    
    return dict(
        param=param,
        guide=guide,
        train_dataset=train_dataset,
        optimizer_kwargs=optimizer_kwargs,
        metadata=dict(
            entry=ENTRY_NAME,
            function_signature=FUNCTION_SIGNATURE,
            eval_class=EVAL_CLASS_NAME,
            benchmark=True,
        )
    )
