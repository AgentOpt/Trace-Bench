#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Autonomous LLM4AD task: machine_learning_pendulum
Generated by convert_llm4ad_benchmark.py

This is a fully self-contained task module that doesn't depend on the original LLM4AD codebase.
"""

# Embedded evaluation code (benchmark)
# Module Name: PendulumEvaluation
# Last Revision: 2025/3/5
# Description: Implements a control strategy for the inverted pendulum swing-up problem. The function
#              selects an appropriate torque based on the pendulum's current state to swing it into an
#              upright position and stabilize it. The goal is to minimize the time required to reach
#              the upright position while ensuring stability. This module is part of the LLM4AD project
#              (https://github.com/Optima-CityU/llm4ad).
#
# Parameters:
#    -   x_position: float - cos(theta), range [-1, 1] (default: None).
#    -   y_position: float - sin(theta), range [-1, 1] (default: None).
#    -   angular_velocity: float - angular velocity of the pendulum, range [-8.0, 8.0] (default: None).
#    -   last_action: float - last torque applied to the pendulum, range [-2.0, 2.0] (default: None).
#    -   timeout_seconds: int - Maximum allowed time (in seconds) for the evaluation process (default: 20).
#
# References:
#   - Brockman, Greg, et al. "Openai gym." arXiv preprint arXiv:1606.01540 (2016).
#
# ------------------------------- Copyright --------------------------------
# Copyright (c) 2025 Optima Group.
#
# Permission is granted to use the LLM4AD platform for research purposes.
# All publications, software, or other works that utilize this platform
# or any part of its codebase must acknowledge the use of "LLM4AD" and
# cite the following reference:
#
# Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang,
# Zhichao Lu, and Qingfu Zhang, "LLM4AD: A Platform for Algorithm Design
# with Large Language Model," arXiv preprint arXiv:2412.17287 (2024).
#
# For inquiries regarding commercial use or licensing, please contact
# http://www.llm4ad.com/contact.html
# --------------------------------------------------------------------------

from __future__ import annotations

from typing import Any
import os, sys
sys.path.insert(0, os.path.dirname(__file__))
import gym
import numpy as np

from llm4ad_loader import Evaluation
# from llm4ad.task.machine_learning.pendulum.template import template_program, task_description  # Template values embedded below

# Embedded template values
template_program = 'import numpy as np\n\ndef choose_action(x: float, y: float, av: float, last_action: float) -> float:\n    """\n    Args:\n        x: cos(theta), between [-1, 1]\n        y: sin(theta), between [-1, 1]\n        av: angular velocity of the pendulum, between [-8.0, 8.0]\n        last_action: the last torque applied to the pendulum, a float between [-2.0, 2.0]\n\n    Return:\n         A float representing the torque to be applied to the pendulum.\n         The value should be in the range of [-2.0, 2.0].\n    """\n    action = np.random.uniform(-2.0, 2.0)\n    return action'
task_description = '("Implement a novel control strategy for the inverted pendulum swing-up problem. The goal is to "'


__all__ = ['PendulumEvaluation']

def evaluate(env: gym.Env, action_select: callable) -> float | None:
    try:
        fitness = []
        # Parallel evaluation 4 times, core=4
        # fitness = Parallel(n_jobs=4)(delayed(evaluate_single)(env, action_select) for _ in range(5))
        for i in range(5):
            fitness.append(evaluate_single(env, action_select))
        fitness = np.mean(fitness)

        return fitness
    except Exception as e:
        return None


def evaluate_single(env: gym.Env, action_select: callable) -> float:
    """Evaluate heuristic function on the pendulum swing-up problem."""

    observation, _ = env.reset()  # initialization
    action = 0.0  # initial action (torque)
    total_reward = 0

    for i in range(env._max_episode_steps + 1):  # protect upper limits
        action = action_select(observation[0],  # cos(theta)
                               observation[1],  # sin(theta)
                               observation[2],  # angular velocity
                               action)  # last action (torque)
        observation, reward, done, truncated, info = env.step([action])
        total_reward += reward

        if done or truncated:
            # self.env.close()
            cos_theta = observation[0]
            sin_theta = observation[1]
            angular_velocity = observation[2]

            # Calculate error terms
            angle_error = abs(1 - cos_theta)  # Distance from vertical (cos(theta) = 1 when upright)
            stability_error = abs(sin_theta)  # Penalize instability

            # Total error
            error = angle_error + stability_error

            # Fitness calculation: ensure fitness > 1 and closer to 1 for better states
            fitness = 1 + error
            if fitness <= 1:
                return -(i + 1) / env._max_episode_steps
            else:
                return -fitness


class PendulumEvaluation(Evaluation):
    """Evaluator for the pendulum swing-up problem."""

    def __init__(self, max_steps=500, timeout_seconds=20, **kwargs):
        """
            Args:
                - 'max_steps' (int): Maximum number of steps allowed per episode in the Pendulum-v1 environment (default is 200).
                - '**kwargs' (dict): Additional keyword arguments passed to the parent class initializer.

            Attributes:
                - 'env' (gym.Env): The Pendulum-v1 environment with a modified maximum episode length.
        """

        super().__init__(
            template_program=template_program,
            task_description=task_description,
            use_numba_accelerate=False,
            timeout_seconds=timeout_seconds
        )

        self.env = None
        self.env = gym.make('Pendulum-v1')
        self.env._max_episode_steps = max_steps

    def evaluate_program(self, program_str: str, callable_func: callable) -> Any | None:
        return evaluate(self.env, callable_func)

# Task configuration for benchmark task
ENTRY_NAME = 'choose_action'
FUNCTION_SIGNATURE = 'def choose_action(...):'
IMPORT_HEADER = 'import math\nimport numpy as np'
TASK_DESCRIPTION = '("Implement a novel control strategy for the inverted pendulum swing-up problem. The goal is to "'
OBJECTIVE_TEXT = 'You are optimizing the implementation of `choose_action` for the LLM4AD task.\\n\\nTask description:\\n("Implement a novel control strategy for the inverted pendulum swing-up problem. The goal is to "\\n\\nYour goal is to return a correct and efficient function whose score (computed by the task evaluator) is as high as possible.'
TEMPLATE_FUNCTION = 'import numpy as np\n\ndef choose_action(x: float, y: float, av: float, last_action: float) -> float:\n    """\n    Args:\n        x: cos(theta), between [-1, 1]\n        y: sin(theta), between [-1, 1]\n        av: angular velocity of the pendulum, between [-8.0, 8.0]\n        last_action: the last torque applied to the pendulum, a float between [-2.0, 2.0]\n\n    Return:\n         A float representing the torque to be applied to the pendulum.\n         The value should be in the range of [-2.0, 2.0].\n    """\n    action = np.random.uniform(-2.0, 2.0)\n    return action'
EVAL_CLASS_NAME = 'PendulumEvaluation'
EVAL_KWARGS = {'max_steps': 500, 'timeout_seconds': 20}

def build_trace_problem(**override_eval_kwargs) -> dict:
    """Build a Trace-ready problem using embedded benchmark evaluator."""
    
    # Create evaluator instance with embedded class
    eval_kwargs_final = EVAL_KWARGS.copy()
    eval_kwargs_final.update(override_eval_kwargs)
    
    evaluator = globals()[EVAL_CLASS_NAME](**eval_kwargs_final)
    
    from llm4ad_loader import AutonomousEvaluatorGuide
    from opto import trace
    
    # Create parameter
    initial_code = TEMPLATE_FUNCTION.strip()
    param = trace.node(initial_code, name='__code', 
                      description=f'The code should start with: {FUNCTION_SIGNATURE}', 
                      trainable=True)
    
    # Create guide using benchmark embedded evaluator
    guide = AutonomousEvaluatorGuide(evaluator, ENTRY_NAME, IMPORT_HEADER, 
                                   timeout=eval_kwargs_final.get('timeout_seconds', 30))
    
    # Create dataset
    train_dataset = dict(
        inputs=[TASK_DESCRIPTION],
        infos=[{'imports': IMPORT_HEADER, 'entry': ENTRY_NAME}]
    )
    
    # Optimizer kwargs
    optimizer_kwargs = dict(
        objective=OBJECTIVE_TEXT,
        memory_size=10
    )
    
    return dict(
        param=param,
        guide=guide,
        train_dataset=train_dataset,
        optimizer_kwargs=optimizer_kwargs,
        metadata=dict(
            entry=ENTRY_NAME,
            function_signature=FUNCTION_SIGNATURE,
            eval_class=EVAL_CLASS_NAME,
            benchmark=True,
        )
    )
